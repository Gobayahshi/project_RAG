import streamlit as st
from langchain.document_loaders import DirectoryLoader  # ë¬¸ì„œ ë¡œë”©
from langchain.text_splitter import RecursiveCharacterTextSplitter  # í…ìŠ¤íŠ¸ ë¶„í• 
from langchain.embeddings import OpenAIEmbeddings  # ì„ë² ë”©
from langchain.vectorstores import FAISS  # ë²¡í„° ì €ì¥ì†Œ
from langchain.chat_models import ChatOpenAI  # LLM
from langchain.chains import ConversationalRetrievalChain
import os

# OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "your-api-key"

st.title("RAG ì‹œìŠ¤í…œ")

# 1. ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬
@st.cache_resource  # ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”
def initialize_rag():
    # ë¬¸ì„œ ë¡œë”©
    loader = DirectoryLoader("your_documents_folder/", glob="*.txt")
    documents = loader.load()
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(documents)
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(splits, embeddings)
    
    # RAG ì²´ì¸ ìƒì„±
    llm = ChatOpenAI(temperature=0)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm,
        vectorstore.as_retriever(),
        return_source_documents=True
    )
    
    return qa_chain

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
qa_chain = initialize_rag()

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‚¬ìš©ì ì…ë ¥
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if user_input:
    # ì§ˆë¬¸ ì²˜ë¦¬
    result = qa_chain({"question": user_input, "chat_history": []})
    
    # ì‘ë‹µ í‘œì‹œ
    st.write("ğŸ¤– ë‹µë³€:", result["answer"])
    
    # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
    st.write("ğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
    for doc in result["source_documents"]:
        st.write("- " + doc.page_content[:200] + "...")
